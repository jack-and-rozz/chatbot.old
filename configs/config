# This config file is assumed to be parsed pyhocon.

# Training hyperparameters.
max_to_keep = 1         # The number of checkpoints kept.
max_epoch = 30          # The number of epochs in training.
learning_rate = 0.0001    # Learning rate.
max_gradient_norm = 5.0 # 
decay_rate = 0.99       #
decay_frequency = 100   #
dropout_rate=0.2        # The dropout ratio in training. 
train_embedding=true    # Whether to retrain the pretrained embeddings or not.
teacher_forcing=false   # Whether to force the model to input the gold target labels in training regardless of the model's choice .
batch_size=10          # Batch size.

# Structure.
num_layers=1                       # The number of layers in MultiRNNCell.
rnn_size=100                        # The dimension of RNN, and other layers.
rnn_type=bidirectional_dynamic_rnn # The name of rnn function in tensorflow.
cell_type=GRUCell                  # The name of RNNCell class in tensorflow.
model_type=Seq2Seq                 # The name of class defined in 'src/core/models/pointernet.py'.
beam_width = 3

#dataset_type=UbuntuDialogueDataset
#dataset_path=dataset/ubuntu-dialogue
dataset_type=DailyDialogDataset
dataset_path=dataset/dailydialog
dataset_info { # The train/valid/test dataset.
  train = {
    path = ${dataset_path}/train.csv
    max_lines = 5000 # The maximum size of training data. if 0, all of the training data will be used.
  }
  valid = {
    path = ${dataset_path}/valid.csv
    max_lines = 10
  }
  test = {
    path = ${dataset_path}/test.csv
    max_lines = 10
  }
}

# Text processing.
utterance_max_len = 25
context_max_len = 1
word_max_len = 0
lowercase=true          # Whether to convert words into lowercase or not.
#cbase = true
cbase = false
wbase = true
w_vocab_size = 20000        # The maximum size of the vocabulary. if 0, use all.
c_vocab_size = 100
w_embedding_size = 300
c_embedding_size = 8
feature_size = 10
char_vocab_path = "dataset/embeddings/char_vocab.english.txt"

#Pretrained embeddings.
embedding_path=dataset/embeddings   # The directory where to put your pretrained embeddings file.
embeddings=${fasttext_300d_en}
fasttext_300d_en{
  path = ${embedding_path}/fasttext/wiki.en.vec
  skip_first=true
}
glove_300d_filtered_en {
  path = ${embedding_path}/glove.840B.300d.txt.filtered
  skip_first=false
}
