{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading a config from configs/config ...\n",
      "Restore the config to checkpoints/tmp/config ...\n",
      "{u'num_train_data': 10000, u'dropout_rate': 0.2, u'num_layers': 1, u'char_vocab_path': u'dataset/embeddings/char_vocab.english.txt', u'decay_frequency': 100, u'w_vocab_size': 30000, u'word_max_len': 20, u'dataset_info': {u'test': {u'path': u'dataset/ubuntu-dialogue/test.csv', u'max_lines': 0}, u'train': {u'path': u'dataset/ubuntu-dialogue/train.csv', u'max_lines': 100000}, u'valid': {u'path': u'dataset/ubuntu-dialogue/valid.csv', u'max_lines': 0}}, u'teacher_forcing': False, u'cell_type': u'GRUCell', u'c_vocab_size': 100, u'max_epoch': 30, u'wbase': True, u'decay_rate': 0.999, u'rnn_type': u'bidirectional_dynamic_rnn', u'c_embedding_size': 8, u'embeddings': [ConfigTree([(u'path', u'dataset/embeddings/glove.840B.300d.txt.filtered'), (u'size', 300), (u'format', u'txt')])], u'utterance_max_len': 20, u'learning_rate': 0.001, u'rnn_size': 50, u'batch_size': 1, u'dataset_type': u'UbuntuDialogueDataset', u'max_gradient_norm': 5.0, u'context_max_len': 4, u'glove_300d_filtered': {u'path': u'dataset/embeddings/glove.840B.300d.txt.filtered', u'size': 300, u'format': u'txt'}, u'lowercase': True, u'w_embedding_size': 300, u'max_to_keep': 1, u'embedding_path': u'dataset/embeddings', u'model_type': u'Seq2Seq', u'cbase': True, u'train_embedding': True}\n",
      "Loading word vocabulary from dataset/ubuntu-dialogue/train.csv.Wvocab30000.lower...\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.054305 sec\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.054305 sec\n",
      "Loading word vocabulary from dataset/ubuntu-dialogue/train.csv.Cvocab100...\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.000428 sec\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.000428 sec\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.000023 sec\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.000023 sec\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.100016 sec\n",
      "[INFO] 2018-03-23 12:32:21 - __init__: 0.100016 sec\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, random, copy, collections, time, re, argparse, commands\n",
    "sys.path.append('src/')\n",
    "sys.stdout = sys.stderr\n",
    "import pyhocon\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from logging import FileHandler\n",
    "import tensorflow as tf\n",
    "from src.main import Manager\n",
    "from utils import common, evaluation, tf_utils\n",
    "from core import models, datasets\n",
    "args = common.dotDict({\n",
    "    'checkpoint_path': 'checkpoints/tmp',\n",
    "    'config_path': 'configs/config',\n",
    "    'cleanup': True,\n",
    "    'log_file': None,\n",
    "    'interactive': True,\n",
    "    'batch_size': 1,\n",
    "})\n",
    "\n",
    "sess = tf.InteractiveSession(graph=tf.Graph())\n",
    "#sess.as_default()\n",
    "manager = Manager(args, sess)\n",
    "config = manager.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = manager.debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset from dataset/ubuntu-dialogue/train.csv ...\n",
      "Preprocessing ...\n",
      "[INFO] 2018-03-23 12:32:48 - load_data: 20.429748 sec\n",
      "[INFO] 2018-03-23 12:32:48 - load_data: 20.429748 sec\n",
      "1 splitting xfonts* out of xfree86* . one upload for the rest of the life and that 's it\n",
      "1 splitting the source package you mean ?\n",
      "1 yes . same binary packages .\n",
      "1 I would prefer to avoid it at this stage . this is something that has gone into XSF svn , I assume ?\n",
      "1 basically each xfree00 upload will not force users to upgrade 000mb of fonts for nothing\n"
     ]
    }
   ],
   "source": [
    "b = batch.next()\n",
    "w_vocab = manager.w_vocab\n",
    "c_vocab = manager.c_vocab\n",
    "sys.stdout = sys.stderr\n",
    "#print 'w_contexts:', b.w_contexts[0]\n",
    "#contexts = [w_vocab.id2sent(x, join=True) for x in b.w_contexts[0]]\n",
    "speaker_changes = b.speaker_changes[0]\n",
    "#print 'c_contexts:', b.c_contexts[0]\n",
    "#print '\\n'.join([c_vocab.id2sent(x, join=True) for x in b.c_contexts[0]])\n",
    "contexts = [c_vocab.id2sent(x, join=True) for x in b.c_contexts[0]]\n",
    "\n",
    "#print 'response:', b.responses[0] \n",
    "responses = w_vocab.id2sent(b.responses[0], join=True)\n",
    "\n",
    "for sc, context in zip(speaker_changes, contexts):\n",
    "    print sc, context\n",
    "print 1, responses\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sys.stdout = sys.stderr\n",
    "nrows = 500\n",
    "train_data = pd.read_csv('dataset/ubuntu-dialogue/train.csv', nrows=nrows)\n",
    "valid_data = pd.read_csv('dataset/ubuntu-dialogue/valid.csv', nrows=nrows)\n",
    "data = train_data\n",
    "batch_size=2\n",
    "utterance_max_len = 20\n",
    "context_max_len = 4\n",
    "_PAD = 0\n",
    "_EOU = '__eou__'\n",
    "_EOT = '__eot__'\n",
    "_URL = '_URL'\n",
    "\n",
    "import random\n",
    "_id=random.randint(0, nrows)\n",
    "from core.datasets import UbuntuDialogueDataset\n",
    "#vocab = UbuntuDialogueDataset(config.dataset_info.train, config.vocab_size, config.lowercase)\n",
    "vocab= UbuntuDialogueDataset.create_vocab_from_data(config.dataset_info.train, config.vocab_size, config.lowercase)\n",
    "dataset = UbuntuDialogueDataset(config.dataset_info, vocab)\n",
    "\n",
    "# if 'Utterance' in data:\n",
    "#     context = [preprocess(x) for x in data['Context']]\n",
    "#     response = [x for x in data['Utterance']]\n",
    "#     print context[_id]\n",
    "#     print response[_id]\n",
    "# else:\n",
    "#     context = [preprocess(x) for x in data['Context']]\n",
    "#     response = [x for x in data['Ground Truth Utterance']]\n",
    "#     n_distractors = 8\n",
    "#     distractors = [] \n",
    "#     for i in xrange(n_distractors):\n",
    "#         distractors.append([preprocess_utterance(x) for x in data['Distractor_%d' % i]])\n",
    "#     distractors = list(zip(*distractors))\n",
    "#     print 'ID', _id\n",
    "#     print 'Context:', context[_id]\n",
    "#     print 'Response:', response[_id]\n",
    "#     for i, d in enumerate(distractors[_id]):\n",
    "#         print 'Distractor_%d:' % i, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-1.4",
   "language": "python",
   "name": "tf-1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
